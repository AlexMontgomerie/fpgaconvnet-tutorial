{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing the previous section, the CNN model is running at floating-point precision by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zy18/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from models import initialize_wrapper\n",
    "\n",
    "dataset_name = 'cifar10'\n",
    "dataset_path = os.path.expanduser(\"~/dataset/cifar10\")\n",
    "model_name = 'vgg16_bn'\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "\n",
    "model_wrapper = initialize_wrapper(dataset_name, model_name,\n",
    "                                    dataset_path, batch_size, workers)\n",
    "                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOAT32 Inference\n",
      "Inference mode: test\n",
      " * Acc@1 94.160 Acc@5 99.710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94.16000366210938, 99.70999908447266)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"FLOAT32 Inference\")\n",
    "model_wrapper.inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fpgaconvnet-torch, we can emulate the quantization effect of weights and activations by calling [`quantize_model`](https://github.com/Yu-Zhewen/fpgaconvnet-torch/blob/main/quantization/utils.py#L208), which will update the parameters in weights and also insert [`QuantAct`](https://github.com/Yu-Zhewen/fpgaconvnet-torch/blob/main/quantization/utils.py#L105) modules between layers to manipulate the activations in the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then run the modified model on a calibration set of data to capture the activatoin dynamic range and decide the scaling factors for activations as well. Finally, the quantized model will be examined on the target validation/test data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantization.utils import QuantMode, quantize_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we would like to quantize both weights and activations into 16-bit fixed point, just run the following code and observe the change in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK FP16 Inference\n",
      "network weight min: tensor(-0.6226, grad_fn=<MinimumBackward0>)\n",
      "network weight max: tensor(0.4982, grad_fn=<MaximumBackward0>)\n",
      "Inference mode: calibrate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zy18/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 100.000 Acc@5 100.000\n",
      "activation min: tensor(-10.6308)\n",
      "activation max: tensor(13.8312)\n",
      "Inference mode: test\n",
      " * Acc@1 94.160 Acc@5 99.710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94.16000366210938, 99.70999908447266)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"NETWORK FP16 Inference\")\n",
    "# reload the model everytime a new quantization mode is tested\n",
    "model_wrapper.load_model()\n",
    "quantize_model(model_wrapper, {\n",
    "                'weight_width': 16, 'data_width': 16, 'mode': QuantMode.NETWORK_FP})\n",
    "model_wrapper.inference(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a different quantization format is desired, we can replace the values in the dict. Note that each time we switch to a different precision, the [`load_model`](https://github.com/Yu-Zhewen/fpgaconvnet-torch/blob/main/models/base.py#L17) will be called to re-initialze the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK FP8 Inference\n",
      "network weight min: tensor(-0.6226, grad_fn=<MinimumBackward0>)\n",
      "network weight max: tensor(0.4982, grad_fn=<MaximumBackward0>)\n",
      "Inference mode: calibrate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zy18/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 100.000 Acc@5 100.000\n",
      "activation min: tensor(-6.9199)\n",
      "activation max: tensor(9.9050)\n",
      "Inference mode: test\n",
      " * Acc@1 13.270 Acc@5 53.130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13.270000457763672, 53.130001068115234)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"NETWORK FP8 Inference\")\n",
    "model_wrapper.load_model()\n",
    "quantize_model(model_wrapper, {\n",
    "                'weight_width': 8, 'data_width': 8, 'mode': QuantMode.NETWORK_FP})\n",
    "model_wrapper.inference(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above two quantizatoin scheme have a shared scaling factor accorss the network. We can also apply block floating-point by changing the [`QuantMode`](https://github.com/Yu-Zhewen/fpgaconvnet-torch/blob/main/quantization/utils.py#L9) to [`CHANNEL_BFP`](https://github.com/Yu-Zhewen/fpgaconvnet-torch/blob/main/quantization/utils.py#L12)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHANNEL BFP8 Inference\n",
      "network weight min: tensor(-0.6226, grad_fn=<MinimumBackward0>)\n",
      "network weight max: tensor(0.4982, grad_fn=<MaximumBackward0>)\n",
      "Inference mode: calibrate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zy18/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 100.000 Acc@5 100.000\n",
      "activation min: tensor(-10.6390)\n",
      "activation max: tensor(13.8376)\n",
      "Inference mode: test\n",
      " * Acc@1 94.090 Acc@5 99.690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(94.08999633789062, 99.69000244140625)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"CHANNEL BFP8 Inference\")\n",
    "model_wrapper.load_model()\n",
    "quantize_model(model_wrapper,  {\n",
    "                'weight_width': 8, 'data_width': 8, 'mode': QuantMode.CHANNEL_BFP})\n",
    "model_wrapper.inference(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are emulating the effect of quantization, so there will be no performance speed-up on CPU/GPU compared with floating-point. Actually, the overhead of emulation will make things run even slower."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
