{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Onnx Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contining the previous section, [fpgaconvnet-torch](https://github.com/Yu-Zhewen/fpgaconvnet-torch/tree/main/models) can compress CNN models using techniques such as quantization, pruning, encoding, and etc. These techniques will impact the performance and resource utilization of the hardware accelerator. Therefore, we annotate these compression-related information as attributes of onnx nodes using the [`generate_onnx_files`](https://github.com/Yu-Zhewen/fpgaconvnet-torch/blob/main/models/utils.py#L147) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zy18/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from models import initialize_wrapper\n",
    "from quantization.utils import QuantMode, quantize_model\n",
    "\n",
    "dataset_name = 'cifar10'\n",
    "dataset_path = os.path.expanduser(\"~/dataset/cifar10\")\n",
    "model_name = 'vgg16_bn'\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "\n",
    "model_wrapper = initialize_wrapper(dataset_name, model_name,\n",
    "                                    dataset_path, batch_size, workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK FP16 Inference\n",
      "network weight min: tensor(-0.6226, grad_fn=<MinimumBackward0>)\n",
      "network weight max: tensor(0.4982, grad_fn=<MaximumBackward0>)\n",
      "Inference mode: calibrate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zy18/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 100.000 Acc@5 100.000\n",
      "activation min: tensor(-10.3823)\n",
      "activation max: tensor(13.2761)\n",
      "Inference mode: test\n",
      " * Acc@1 94.170 Acc@5 99.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zy18/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fp16/vgg16_bn.onnx'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"NETWORK FP16 Inference\")\n",
    "# reload the model everytime a new quantization mode is tested\n",
    "model_wrapper.load_model()\n",
    "quantize_model(model_wrapper, {\n",
    "                'weight_width': 16, 'data_width': 16, 'mode': QuantMode.NETWORK_FP})\n",
    "model_wrapper.inference(\"test\")\n",
    "model_wrapper.generate_onnx_files(\"fp16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the exported onnx graph in [`netron`](https://github.com/lutzroeder/netron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'fp16/vgg16_bn.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import netron\n",
    "\n",
    "netron.start('fp16/vgg16_bn.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take convolutional layers for example, in each layer's attributes, there are `acc_width`, `block_floating_point`, `data_width` and `weight_width` related to the quantization. If an onnx file does not contain these customized quantization information, fpgaconvnet-model will interpret the precision fp16 by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, in case you don't need the accuracy result, you can directly modify the `sideband_info` attribute of the `model_wrapper` object to switch a precision. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zy18/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n",
      "Using cache found in /home/zy18/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'bfp8/vgg16_bn.onnx' at http://localhost:8081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8081)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapper.load_model()\n",
    "model_wrapper.sideband_info['quantization'] = {\n",
    "                'weight_width': 8, 'data_width': 8, 'mode': QuantMode.CHANNEL_BFP}\n",
    "model_wrapper.generate_onnx_files(\"bfp8\")\n",
    "netron.start('bfp8/vgg16_bn.onnx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
