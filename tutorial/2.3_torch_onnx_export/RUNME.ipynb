{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Onnx Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contining the previous section, [fpgaconvnet-torch](https://github.com/Yu-Zhewen/fpgaconvnet-torch/tree/main/models) can compress CNN models using techniques such as quantization, pruning, encoding, and etc. These techniques will impact the performance and resource utilization of the hardware accelerator. Therefore, we annotate these compression-related information as attributes of onnx nodes using the [`generate_onnx_files`](https://github.com/Yu-Zhewen/fpgaconvnet-torch/blob/main/models/utils.py#L147) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zy18/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from models import initialize_wrapper\n",
    "from quantization.utils import QuantMode, quantize_model\n",
    "\n",
    "dataset_name = 'cifar10'\n",
    "dataset_path = os.path.expanduser(\"~/dataset/cifar10\")\n",
    "model_name = 'vgg16_bn'\n",
    "batch_size = 64\n",
    "workers = 4\n",
    "\n",
    "model_wrapper = initialize_wrapper(dataset_name, model_name,\n",
    "                                    dataset_path, batch_size, workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NETWORK FP16 Inference\n",
      "network weight min: tensor(-0.6226, grad_fn=<MinimumBackward0>)\n",
      "network weight max: tensor(0.4982, grad_fn=<MaximumBackward0>)\n",
      "Inference mode: calibrate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zy18/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Acc@1 100.000 Acc@5 100.000\n",
      "activation min: tensor(-10.3823)\n",
      "activation max: tensor(13.2761)\n",
      "Inference mode: test\n",
      " * Acc@1 94.170 Acc@5 99.710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zy18/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fp16/vgg16_bn.onnx'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"NETWORK FP16 Inference\")\n",
    "# reload the model everytime a new quantization mode is tested\n",
    "model_wrapper.load_model()\n",
    "quantize_model(model_wrapper, {\n",
    "                'weight_width': 16, 'data_width': 16, 'mode': QuantMode.NETWORK_FP})\n",
    "model_wrapper.inference(\"test\")\n",
    "model_wrapper.generate_onnx_files(\"fp16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the exported onnx graph in [`netron`](https://github.com/lutzroeder/netron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'fp16/vgg16_bn.onnx' at http://localhost:8080\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8080)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import netron\n",
    "\n",
    "netron.start('fp16/vgg16_bn.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take convolutional layers for example, in each layer's attributes, there are `acc_width`, `block_floating_point`, `data_width` and `weight_width` related to the quantization. If an onnx file does not contain these customized information, fpgaconvnet-model will interpret the precision fp16 by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you want to skip fpgaconvnet-torch, and generate the onnx file from your own PyTorch/Tensorflow codebase, make sure you annotate your onnx files the same way as fpgaconvnet-torch did so that the rest of our tool can understand it. Detailed can be found in these [annotation passes](https://github.com/Yu-Zhewen/fpgaconvnet-torch/blob/main/models/utils.py#L58)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, if you don't need the accuracy result, you can directly modify the `sideband_info` attribute of the `model_wrapper` object to switch a precision. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/zy18/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n",
      "Using cache found in /home/zy18/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'bfp8/vgg16_bn.onnx' at http://localhost:8081\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('localhost', 8081)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_wrapper.load_model()\n",
    "model_wrapper.sideband_info['quantization'] = {\n",
    "                'weight_width': 8, 'data_width': 8, 'mode': QuantMode.CHANNEL_BFP}\n",
    "model_wrapper.generate_onnx_files(\"bfp8\")\n",
    "netron.start('bfp8/vgg16_bn.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
