{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Onnx Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this section, let's start looking at [fpgaconvnet-model](https://github.com/AlexMontgomerie/fpgaconvnet-model), which converts the onnx graph to a hardware accelerator configuration, providing estimated metrics with the help of performance and resource modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entry point of fpgaconvnet-model is the [`Parser`](https://github.com/AlexMontgomerie/fpgaconvnet-model/blob/dev-petros/fpgaconvnet/parser/Parser.py#L26) that translates onnx to our own Intermediate Representation as Synchronous Data Flow (SDF) graph which is built on the [networkx](https://networkx.org/) package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now either use the onnx file generated in the previous tutorial or use the one we provide, run the following code which will return a networkx graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL WARNING: node Flatten_31 is skipped in hardware\n"
     ]
    }
   ],
   "source": [
    "from fpgaconvnet.parser.Parser import Parser\n",
    "\n",
    "onnx_path = \"fp16/vgg16_bn.onnx\"\n",
    "\n",
    "parser = Parser(custom_onnx=True, batch_size=1)\n",
    "net = parser.onnx_to_fpgaconvnet(onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated networkx graph has the hierarchy of:\n",
    "\n",
    "* [`Network`](https://github.com/AlexMontgomerie/fpgaconvnet-model/blob/dev-petros/fpgaconvnet/models/network/Network.py), corresponds to the whole onnx graph.\n",
    "* [`Partition`](https://github.com/AlexMontgomerie/fpgaconvnet-model/blob/dev-petros/fpgaconvnet/models/partition/Partition.py), a subgraph that eventually will be converted to a bitstream. partitions can be either placed across multi-FPGA devices or scheduled on a single divice through reconfiguration.\n",
    "* [`Layer`](https://github.com/AlexMontgomerie/fpgaconvnet-model/blob/dev-petros/fpgaconvnet/models/layers/Layer.py), corresponds to a node in onnx, and the mapping between them can be found [here](https://github.com/AlexMontgomerie/fpgaconvnet-model/blob/dev-petros/fpgaconvnet/parser/Parser.py#L150). \n",
    "* [`Module`](https://github.com/AlexMontgomerie/fpgaconvnet-model/blob/dev-petros/fpgaconvnet/models/modules/Module.py), corresponds to the hardware module implemented in [fpgaconvnet-hls](https://github.com/AlexMontgomerie/fpgaconvnet-hls/tree/main/fpgaconvnet/hls/hardware)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to iterative over the generated networkx graph, please run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <fpgaconvnet.models.partition.Partition.Partition object at 0x7fccc778b820>\n",
      "0 Conv_0 LAYER_TYPE.Convolution\n",
      "1 Relu_1 LAYER_TYPE.ReLU\n",
      "2 Conv_2 LAYER_TYPE.Convolution\n",
      "3 Relu_3 LAYER_TYPE.ReLU\n",
      "4 MaxPool_4 LAYER_TYPE.Pooling\n",
      "5 Conv_5 LAYER_TYPE.Convolution\n",
      "6 Relu_6 LAYER_TYPE.ReLU\n",
      "7 Conv_7 LAYER_TYPE.Convolution\n",
      "8 Relu_8 LAYER_TYPE.ReLU\n",
      "9 MaxPool_9 LAYER_TYPE.Pooling\n",
      "10 Conv_10 LAYER_TYPE.Convolution\n",
      "11 Relu_11 LAYER_TYPE.ReLU\n",
      "12 Conv_12 LAYER_TYPE.Convolution\n",
      "13 Relu_13 LAYER_TYPE.ReLU\n",
      "14 Conv_14 LAYER_TYPE.Convolution\n",
      "15 Relu_15 LAYER_TYPE.ReLU\n",
      "16 MaxPool_16 LAYER_TYPE.Pooling\n",
      "17 Conv_17 LAYER_TYPE.Convolution\n",
      "18 Relu_18 LAYER_TYPE.ReLU\n",
      "19 Conv_19 LAYER_TYPE.Convolution\n",
      "20 Relu_20 LAYER_TYPE.ReLU\n",
      "21 Conv_21 LAYER_TYPE.Convolution\n",
      "22 Relu_22 LAYER_TYPE.ReLU\n",
      "23 MaxPool_23 LAYER_TYPE.Pooling\n",
      "24 Conv_24 LAYER_TYPE.Convolution\n",
      "25 Relu_25 LAYER_TYPE.ReLU\n",
      "26 Conv_26 LAYER_TYPE.Convolution\n",
      "27 Relu_27 LAYER_TYPE.ReLU\n",
      "28 Conv_28 LAYER_TYPE.Convolution\n",
      "29 Relu_29 LAYER_TYPE.ReLU\n",
      "30 GlobalMaxPool_30 LAYER_TYPE.GlobalPooling\n",
      "31 Gemm_32 LAYER_TYPE.InnerProduct\n",
      "32 Relu_33 LAYER_TYPE.ReLU\n",
      "33 Gemm_34 LAYER_TYPE.InnerProduct\n",
      "34 Relu_35 LAYER_TYPE.ReLU\n",
      "35 Gemm_36 LAYER_TYPE.InnerProduct\n"
     ]
    }
   ],
   "source": [
    "for i, partition in enumerate(net.partitions):\n",
    "    print(i, partition)\n",
    "    for j, layer_name in enumerate(partition.graph.nodes):\n",
    "        layer_type = partition.graph.nodes[layer_name][\"type\"]\n",
    "        layer = partition.graph.nodes[layer_name][\"hw\"]\n",
    "        print(j, layer_name, layer_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `net` object has a single partition by default, which can also be accessed by `net.partitions[0]`. Regarding the layers, `partition.graph.nodes[layer_name]` is a dict as defined [`here`](https://github.com/AlexMontgomerie/fpgaconvnet-model/blob/dev-petros/fpgaconvnet/parser/onnx/parse.py#L67)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['type', 'onnx_node', 'onnx_input', 'onnx_output', 'attr', 'hw', 'inputs'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.partitions[0].graph.nodes[\"Conv_0\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its `\"type\"` is an enumeration object which is defined [`here`](https://github.com/AlexMontgomerie/fpgaconvnet-model/blob/dev-petros/fpgaconvnet/tools/layer_enum.py#L6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<LAYER_TYPE.Convolution: 4>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.partitions[0].graph.nodes[\"Conv_0\"][\"type\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while the actual `Layer` object can be accessed by `\"hw\"`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionLayer(_rows=32, _cols=32, _channels=3, _coarse_in=1, _coarse_out=1, input_compression_ratio=[1.0, 1.0, 1.0], output_compression_ratio=[1.0], mem_bw_in=100.0, mem_bw_out=100.0, data_t=FixedPoint(width=16, binary_point=8), buffer_depth=2, modules=OrderedDict([('pad', Pad(rows=32, cols=32, channels=3, data_width=16, rsc_coef={'FF': [], 'LUT': [], 'DSP': [], 'BRAM': []}, pad_top=1, pad_bottom=1, pad_left=1, pad_right=1, backend='chisel', regression_model='linear_regression', streams=1, latency_mode=False, block=False)), ('sliding_window', SlidingWindow(rows=34, cols=34, channels=3, data_width=16, rsc_coef={'Logic_LUT': array([6.46840521, 0.        , 0.        , 3.92659388, 2.45084112]), 'LUT_RAM': array([ 0.07302216,  0.        , 14.23619095]), 'LUT_SR': array([0.]), 'FF': array([ 0.        ,  0.        , 20.77153538,  0.        ,  0.        ,\n",
       "        0.        ,  5.21236925,  3.44020551,  0.        ,  0.        ]), 'DSP': array([0.]), 'BRAM36': array([0.]), 'BRAM18': array([0.])}, kernel_size=[3, 3], stride=[1, 1], pad_top=0, pad_right=0, pad_bottom=0, pad_left=0, backend='chisel', regression_model='linear_regression', streams=1)), ('squeeze', Squeeze(rows=32, cols=32, channels=3, data_width=16, rsc_coef={'Logic_LUT': array([  0.        ,   2.22688964,   0.        ,   0.        ,\n",
       "       235.5859881 ,   0.        ,   3.29301748,   0.        ]), 'LUT_RAM': array([48.82925296]), 'LUT_SR': array([0]), 'FF': array([   0.        ,   15.87391647,    3.48485944,    0.        ,\n",
       "          5.69486761, 1084.90151568]), 'DSP': array([0]), 'BRAM36': array([0]), 'BRAM18': array([0])}, coarse_in=9, coarse_out=1, backend='chisel', regression_model='linear_regression', streams=1)), ('fork', Fork(rows=32, cols=32, channels=3, data_width=16, rsc_coef={'Logic_LUT': array([0.00000000e+00, 5.00000000e-01, 0.00000000e+00, 1.85479216e-18,\n",
       "       5.00000000e+00]), 'LUT_RAM': array([0.]), 'LUT_SR': array([0.]), 'FF': array([12.,  0.,  3.]), 'DSP': array([0.]), 'BRAM36': array([0.]), 'BRAM18': array([0.])}, kernel_size=[1, 1], coarse=1, backend='chisel', regression_model='linear_regression', streams=1)), ('vector_dot', VectorDot(rows=32, cols=32, channels=27, data_width=16, rsc_coef={'Logic_LUT': array([2.57125885, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.97287269, 0.        , 0.        , 0.        ]), 'LUT_RAM': array([4.52464937, 0.        ]), 'LUT_SR': array([0.16773743]), 'FF': array([ 0.        ,  0.        ,  0.        ,  0.        , 21.36862368,\n",
       "        1.48600702,  0.18202488,  0.        ]), 'DSP': array([1.]), 'BRAM36': array([0.]), 'BRAM18': array([0.])}, filters=64, fine=1, backend='chisel', regression_model='linear_regression', weight_width=16, acc_width=32, streams=1)), ('accum', Accum(rows=32, cols=32, channels=27, data_width=32, rsc_coef={'Logic_LUT': array([0.85353288, 0.        , 3.51035984, 0.        , 0.        ,\n",
       "       0.        , 0.        ]), 'LUT_RAM': array([13.67379136,  0.01678309,  0.92521122,  0.        ]), 'LUT_SR': array([0.]), 'FF': array([0.        , 3.20979382, 0.        , 0.        , 0.00732597,\n",
       "       0.28885062, 0.        ]), 'DSP': array([0.]), 'BRAM36': array([0.]), 'BRAM18': array([0.])}, filters=64, groups=1, skip_all_zero_window=False, sparsity_hist=None, backend='chisel', regression_model='linear_regression', streams=1)), ('glue', Glue(rows=32, cols=32, channels=1, data_width=32, rsc_coef={'Logic_LUT': array([1.26165591, 0.        , 0.        , 0.        ]), 'LUT_RAM': array([8.87874832, 0.        ]), 'LUT_SR': array([0.16089866, 0.19529144]), 'FF': array([9.53751151, 0.36080769, 0.        , 0.05106319, 1.0280541 ,\n",
       "       0.        ]), 'DSP': array([0.]), 'BRAM36': array([0.]), 'BRAM18': array([0.])}, filters=64, coarse_in=1, coarse_out=1, coarse_group=1, backend='chisel', regression_model='linear_regression', streams=1)), ('bias', Bias(rows=32, cols=32, channels=1, data_width=16, rsc_coef={'Logic_LUT': array([0.8125]), 'LUT_RAM': array([0.]), 'LUT_SR': array([0.]), 'FF': array([2.1875, 0.    , 0.    ]), 'DSP': array([0.]), 'BRAM36': array([0.]), 'BRAM18': array([0.])}, filters=64, biases_width=32, backend='chisel', regression_model='linear_regression', streams=1)), ('shift_scale', ShiftScale(rows=32, cols=32, channels=1, data_width=16, rsc_coef={'FF': [], 'LUT': [], 'DSP': [], 'BRAM': []}, filters=64, biases_width=32, backend='chisel', regression_model='linear_regression', streams=1))]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.partitions[0].graph.nodes[\"Conv_0\"][\"hw\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the `Module`s inside the `Layer` can be accessed by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('pad',\n",
       "              Pad(rows=32, cols=32, channels=3, data_width=16, rsc_coef={'FF': [], 'LUT': [], 'DSP': [], 'BRAM': []}, pad_top=1, pad_bottom=1, pad_left=1, pad_right=1, backend='chisel', regression_model='linear_regression', streams=1, latency_mode=False, block=False)),\n",
       "             ('sliding_window',\n",
       "              SlidingWindow(rows=34, cols=34, channels=3, data_width=16, rsc_coef={'Logic_LUT': array([6.46840521, 0.        , 0.        , 3.92659388, 2.45084112]), 'LUT_RAM': array([ 0.07302216,  0.        , 14.23619095]), 'LUT_SR': array([0.]), 'FF': array([ 0.        ,  0.        , 20.77153538,  0.        ,  0.        ,\n",
       "                      0.        ,  5.21236925,  3.44020551,  0.        ,  0.        ]), 'DSP': array([0.]), 'BRAM36': array([0.]), 'BRAM18': array([0.])}, kernel_size=[3, 3], stride=[1, 1], pad_top=0, pad_right=0, pad_bottom=0, pad_left=0, backend='chisel', regression_model='linear_regression', streams=1)),\n",
       "             ('squeeze',\n",
       "              Squeeze(rows=32, cols=32, channels=3, data_width=16, rsc_coef={'Logic_LUT': array([  0.        ,   2.22688964,   0.        ,   0.        ,\n",
       "                     235.5859881 ,   0.        ,   3.29301748,   0.        ]), 'LUT_RAM': array([48.82925296]), 'LUT_SR': array([0]), 'FF': array([   0.        ,   15.87391647,    3.48485944,    0.        ,\n",
       "                        5.69486761, 1084.90151568]), 'DSP': array([0]), 'BRAM36': array([0]), 'BRAM18': array([0])}, coarse_in=9, coarse_out=1, backend='chisel', regression_model='linear_regression', streams=1)),\n",
       "             ('fork',\n",
       "              Fork(rows=32, cols=32, channels=3, data_width=16, rsc_coef={'Logic_LUT': array([0.00000000e+00, 5.00000000e-01, 0.00000000e+00, 1.85479216e-18,\n",
       "                     5.00000000e+00]), 'LUT_RAM': array([0.]), 'LUT_SR': array([0.]), 'FF': array([12.,  0.,  3.]), 'DSP': array([0.]), 'BRAM36': array([0.]), 'BRAM18': array([0.])}, kernel_size=[1, 1], coarse=1, backend='chisel', regression_model='linear_regression', streams=1)),\n",
       "             ('vector_dot',\n",
       "              VectorDot(rows=32, cols=32, channels=27, data_width=16, rsc_coef={'Logic_LUT': array([2.57125885, 0.        , 0.        , 0.        , 0.        ,\n",
       "                     0.97287269, 0.        , 0.        , 0.        ]), 'LUT_RAM': array([4.52464937, 0.        ]), 'LUT_SR': array([0.16773743]), 'FF': array([ 0.        ,  0.        ,  0.        ,  0.        , 21.36862368,\n",
       "                      1.48600702,  0.18202488,  0.        ]), 'DSP': array([1.]), 'BRAM36': array([0.]), 'BRAM18': array([0.])}, filters=64, fine=1, backend='chisel', regression_model='linear_regression', weight_width=16, acc_width=32, streams=1)),\n",
       "             ('accum',\n",
       "              Accum(rows=32, cols=32, channels=27, data_width=32, rsc_coef={'Logic_LUT': array([0.85353288, 0.        , 3.51035984, 0.        , 0.        ,\n",
       "                     0.        , 0.        ]), 'LUT_RAM': array([13.67379136,  0.01678309,  0.92521122,  0.        ]), 'LUT_SR': array([0.]), 'FF': array([0.        , 3.20979382, 0.        , 0.        , 0.00732597,\n",
       "                     0.28885062, 0.        ]), 'DSP': array([0.]), 'BRAM36': array([0.]), 'BRAM18': array([0.])}, filters=64, groups=1, skip_all_zero_window=False, sparsity_hist=None, backend='chisel', regression_model='linear_regression', streams=1)),\n",
       "             ('glue',\n",
       "              Glue(rows=32, cols=32, channels=1, data_width=32, rsc_coef={'Logic_LUT': array([1.26165591, 0.        , 0.        , 0.        ]), 'LUT_RAM': array([8.87874832, 0.        ]), 'LUT_SR': array([0.16089866, 0.19529144]), 'FF': array([9.53751151, 0.36080769, 0.        , 0.05106319, 1.0280541 ,\n",
       "                     0.        ]), 'DSP': array([0.]), 'BRAM36': array([0.]), 'BRAM18': array([0.])}, filters=64, coarse_in=1, coarse_out=1, coarse_group=1, backend='chisel', regression_model='linear_regression', streams=1)),\n",
       "             ('bias',\n",
       "              Bias(rows=32, cols=32, channels=1, data_width=16, rsc_coef={'Logic_LUT': array([0.8125]), 'LUT_RAM': array([0.]), 'LUT_SR': array([0.]), 'FF': array([2.1875, 0.    , 0.    ]), 'DSP': array([0.]), 'BRAM36': array([0.]), 'BRAM18': array([0.])}, filters=64, biases_width=32, backend='chisel', regression_model='linear_regression', streams=1)),\n",
       "             ('shift_scale',\n",
       "              ShiftScale(rows=32, cols=32, channels=1, data_width=16, rsc_coef={'FF': [], 'LUT': [], 'DSP': [], 'BRAM': []}, filters=64, biases_width=32, backend='chisel', regression_model='linear_regression', streams=1))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.partitions[0].graph.nodes[\"Conv_0\"][\"hw\"].modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is a dict with module name as the key and module object as the value. To learn more about the [`Network`](https://github.com/AlexMontgomerie/fpgaconvnet-model/blob/dev-petros/fpgaconvnet/models/network/Network.py), [`Partition`](https://github.com/AlexMontgomerie/fpgaconvnet-model/blob/dev-petros/fpgaconvnet/models/partition/Partition.py), [`Layer`](https://github.com/AlexMontgomerie/fpgaconvnet-model/blob/dev-petros/fpgaconvnet/models/layers/Layer.py), and [`Module`](https://github.com/AlexMontgomerie/fpgaconvnet-model/blob/dev-petros/fpgaconvnet/models/modules/Module.py), please continue the rest of the tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
