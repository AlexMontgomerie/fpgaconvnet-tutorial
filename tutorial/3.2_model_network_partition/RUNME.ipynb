{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Network Partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, let us have a more detailed look at the `Network` and `Partition` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL WARNING: node Flatten_31 is skipped in hardware\n"
     ]
    }
   ],
   "source": [
    "from fpgaconvnet.parser.Parser import Parser\n",
    "\n",
    "onnx_path = \"../3.1_model_onnx_parser/fp16/vgg16_bn.onnx\"\n",
    "parser = Parser(custom_onnx=True, batch_size=1)\n",
    "net = parser.onnx_to_fpgaconvnet(onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the network performance estimation, please run the following code. Here we assume that we are using a single FPGA device clocked @200 MHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 1\n",
      "Cycles:  199901582\n",
      "Latency (seconds):  0.9995079149121093\n",
      "Throughput (frames per second):  1.0004923273548403\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of partitions:\", len(net.partitions))\n",
    "\n",
    "multi_device = False\n",
    "freq = 200 # MHz\n",
    "reconf_time = 0.08255 # second\n",
    "\n",
    "cycles = int(net.get_cycle(multi_device))\n",
    "latency = net.get_latency(freq, multi_device, reconf_time)\n",
    "throughput = net.get_throughput(freq, multi_device, reconf_time)\n",
    "\n",
    "print(\"Cycles: \", cycles)\n",
    "print(\"Latency (seconds): \", latency)\n",
    "print(\"Throughput (frames per second): \", throughput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emmm... The design seems to be a bit slow, but don't worry this is just an example to show you how to obtain these estimations at network level. In fact, for the `net` object generated by `Parser`, the computation inside each layer is set as fully sequential mode. We'll show how to improve the performance in the later part of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the [`net.get_cycle`](https://github.com/AlexMontgomerie/fpgaconvnet-model/blob/dev-petros/fpgaconvnet/models/network/Network.py#L121) function, since we are sequentially scheduling partitions are a single deive, the total network cycle the sum of invidual partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, to obtain the resource estimiation, we can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, resource:  {'FF': 38956, 'LUT': 63555, 'DSP': 17, 'BRAM': 15002, 'URAM': 0}\n"
     ]
    }
   ],
   "source": [
    "for i, partition in enumerate(net.partitions):\n",
    "    print(f\"{i}, resource: \", partition.get_resource_usage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh!!! That's a lot of BRAM, and why is that? In the default setup of streaming, dataflow accelerator stores all the weights on-chip all the time. For the VGG16 model that we are looking at, it has around 15M parameters. When the model is quantized to W16A16, the required memory size will be roughly 244Mb. Given that each Xilinx BRAM has the capacity of 18Kb, 244Mb/18Kb=13000 BRAMs plus there will some overhead elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again there is no need to worry about this at the moment. We can either use a smaller model, or from hardware persepective, fpgaConvNet supports [device reconfiguration](https://github.com/AlexMontgomerie/fpgaconvnet-optimiser/blob/dev-petros/fpgaconvnet/optimiser/transforms/partition.py), [weights reloading](https://github.com/AlexMontgomerie/fpgaconvnet-optimiser/blob/dev-petros/fpgaconvnet/optimiser/transforms/weights_reloading.py) and [weights streaming](https://github.com/AlexMontgomerie/fpgaconvnet-optimiser/blob/dev-petros/fpgaconvnet/optimiser/solvers/greedy_partition.py#L366) to deal with this problem, and we will talk about them later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, change the precision of the model will also make a difference. Now let's load the onnx model which is annotate with BFP8 quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL WARNING: node Flatten_31 is skipped in hardware\n",
      "0, resource:  {'FF': 33187, 'LUT': 48492, 'DSP': 31.5, 'BRAM': 7556, 'URAM': 0}\n"
     ]
    }
   ],
   "source": [
    "onnx_path = \"../3.1_model_onnx_parser/bfp8/vgg16_bn.onnx\"\n",
    "parser = Parser(custom_onnx=True, batch_size=1)\n",
    "net = parser.onnx_to_fpgaconvnet(onnx_path)\n",
    "for i, partition in enumerate(net.partitions):\n",
    "    print(f\"{i}, resource: \", partition.get_resource_usage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
