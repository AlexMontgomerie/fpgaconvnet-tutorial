{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SDK and Host Tutorial\n",
    "## Vivado SDK\n",
    "In the HLS part, we have generated the hardware IP. The hardware IP is integrated into the block design as PL. Therefore, we need the relevant PS to interact with PL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate an IP `hw` and allocate memory addresses for input and output.\n",
    "\n",
    "```C++\n",
    "XFpgaconvnet_ip hw;\n",
    "\n",
    "// Set IO addresses\n",
    "XFpgaconvnet_ip_Initialize(&hw, XPAR_FPGACONVNET_IP_0_DEVICE_ID);\n",
    "XFpgaconvnet_ip_Set_fpgaconvnet_in_0(&hw, (u32) input);\n",
    "XFpgaconvnet_ip_Set_fpgaconvnet_out_0(&hw, (u32) output);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cin` is the input. In the design, the input is sent from the host through UART. In the simple end-to-end example, the `coarse_in` parameter is set to 1; hence every input has 16 bits of \"meaningful\" data, with other bits set to `0`. In this case, the input is converted to the `u64` type, `OR`ed with a bit mask, and stored in an array.\n",
    "\n",
    "`Xil_DCacheFlush()` flushes the cache; otherwise, the content will be wrongly written to memory.\n",
    "\n",
    "```C++\n",
    "// Parse and load input featuremap from UART\n",
    "for (int i = 0; i < INPUT_SHAPE; i++) {\n",
    "    std::cin >> in;\n",
    "    input[i] = ((u64) std::stoi(in)) & 0x000000000000ffff;\n",
    "}\n",
    "\n",
    "usleep(100000);\n",
    "\n",
    "// Flush cache\n",
    "Xil_DCacheFlush();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the input is loaded, we run the IP. The IP is run multiple times, and the running time is measured. Running `RUNS` times ensures a more precise measurement of time.\n",
    "\n",
    "```C++\n",
    "XTime t_start, t_end;\n",
    "XTime_GetTime(&t_start);\n",
    "\n",
    "for(int i = 0; i < RUNS; i++) {\n",
    "    XFpgaconvnet_ip_Start(&hw);\n",
    "\n",
    "    // Wait for IP to finish\n",
    "    while (!XFpgaconvnet_ip_IsReady(&hw));\n",
    "}\n",
    "\n",
    "XTime_GetTime(&t_end);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the output is dumped, we first invalidate the cache to avoid any potential effect on the output. \n",
    "\n",
    "The `coarse_out` parameter is 4; hence every 64-bit word contains four unsigned 16-bit output values. In a byte-addressing system, each 16-bit value occupies two memory addresses. `output` represents the absolute address starting at which the output values are stored. For each output, the program reads from two memory addresses. The output is received by the host through UART, the same way the input is transmitted.\n",
    "\n",
    "```C++\n",
    "// Invalidate cache\n",
    "Xil_DCacheInvalidate();\n",
    "\n",
    "// Dump output\n",
    "for (int i = 0; i < OUTPUT_SHAPE; i++) {\n",
    "    // Load address, the processor is byte addressable, hence each address is 8 bits\n",
    "    std::cout << Xil_In16(((u64) output) + 2 * i) << \" \";\n",
    "    usleep(50);\n",
    "}\n",
    "std::cout << '\\n';\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eventually, we output the information of the execution and flush the cache again in preparation for following executions. \n",
    "\n",
    "```C++\n",
    "float t_run = (t_end - t_start)* 1000000. /COUNTS_PER_SECOND;\n",
    "std::cout<< \"Completed \" << RUNS << \" Runs.  Time taken (us): \" << (int) t_run << \"  Rate (img/s): \" << 1/t_run * RUNS * 1000000 << \"\\n\";\n",
    "Xil_DCacheFlush();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host Code \n",
    "The host code extracts the input image (in the simple end-to-end example, from the MNIST dataset), uses a hardware-accelerated method to run inference, and compares the result with the software method to verify the implementation.\n",
    "\n",
    "Firstly, the host program identifies the index of the image in the dataset selected for comparison by extracting it from the command line. [`get_MNIST_image`](https://github.com/AlexMontgomerie/fpgaconvnet-tutorial/blob/main/tutorial/1_simple_end_to_end/hardware-tutorial-assets/host-code/tutorial_library.py#L7) grabs the MNIST image from the dataset and normalizes the data. Afterwards, the image is [sent](https://github.com/AlexMontgomerie/fpgaconvnet-tutorial/blob/main/tutorial/1_simple_end_to_end/hardware-tutorial-assets/host-code/tutorial_library.py#L42) to the FPGA using UART. `mnist_image` is a 4D array, with four dimensions indicating [Batch Size], [Channel], [Height], and [Width]. For the MNIST dataset, the first two parameters are 1. The data is multiplied by 256 for fixed-point calculation.\n",
    "\n",
    "After the FPGA processes the data, [receive_string](https://github.com/AlexMontgomerie/fpgaconvnet-tutorial/blob/main/tutorial/1_simple_end_to_end/hardware-tutorial-assets/host-code/tutorial_library.py#L77) receives the message, and [receive_array](https://github.com/AlexMontgomerie/fpgaconvnet-tutorial/blob/main/tutorial/1_simple_end_to_end/hardware-tutorial-assets/host-code/tutorial_library.py#L85) receives the output data. The output data is flattened for display.\n",
    "\n",
    "For software reference, the [run_inference](https://github.com/AlexMontgomerie/fpgaconvnet-tutorial/blob/main/tutorial/1_simple_end_to_end/hardware-tutorial-assets/host-code/tutorial_library.py#L37) function runs inference using `onnxruntime`, and the reference output data is flattened.\n",
    "\n",
    "Unlike `mnist_image`, the 4D output of fpgaConvNet indicates [Batch Size], [Height], [Width], and [Channel], so comparison requires transposing the array. The two outputs are compared, and the mean squared error is computed. A small error verifies the functionality of the hardware method. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
